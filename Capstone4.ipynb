{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from math import ceil"
      ],
      "metadata": {
        "id": "FAkRUwH7nnMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/chicken_photos/',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/chicken_photos/',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation')\n",
        "\n",
        "# Calculate steps per epoch and validation steps\n",
        "steps_per_epoch = train_generator.n // train_generator.batch_size\n",
        "validation_steps = validation_generator.n // validation_generator.batch_size\n",
        "\n",
        "# Load pre-trained VGG16 model\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "vgg16_base.trainable = False  # Freeze the convolutional base\n",
        "\n",
        "# Add custom layers\n",
        "vgg16_model = models.Sequential([\n",
        "    vgg16_base,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base.trainable = False  # Freeze the convolutional base\n",
        "\n",
        "# Add custom layers\n",
        "resnet50_model = models.Sequential([\n",
        "    resnet50_base,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile VGG16 model\n",
        "vgg16_model.compile(loss='binary_crossentropy',\n",
        "                    optimizer='adam',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Compile ResNet50 model\n",
        "resnet50_model.compile(loss='binary_crossentropy',\n",
        "                       optimizer='adam',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# Train the VGG16 model\n",
        "vgg16_history = vgg16_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps)\n",
        "\n",
        "# Train the ResNet50 model\n",
        "resnet50_history = resnet50_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps)\n"
      ],
      "metadata": {
        "id": "xM-ApKFvne1C",
        "outputId": "e694bad2-8a72-4644-b357-477939610ba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 779 images belonging to 2 classes.\n",
            "Found 193 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "24/24 [==============================] - 712s 29s/step - loss: 0.4888 - accuracy: 0.8849 - val_loss: 0.1331 - val_accuracy: 0.9844\n",
            "Epoch 2/10\n",
            "24/24 [==============================] - 676s 28s/step - loss: 0.0213 - accuracy: 0.9920 - val_loss: 0.1705 - val_accuracy: 0.9740\n",
            "Epoch 3/10\n",
            "24/24 [==============================] - 742s 31s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.9844\n",
            "Epoch 4/10\n",
            "24/24 [==============================] - 684s 29s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9844\n",
            "Epoch 5/10\n",
            "24/24 [==============================] - 678s 28s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9844\n",
            "Epoch 6/10\n",
            "24/24 [==============================] - 677s 28s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1557 - val_accuracy: 0.9844\n",
            "Epoch 7/10\n",
            "24/24 [==============================] - 674s 28s/step - loss: 9.4978e-04 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9844\n",
            "Epoch 8/10\n",
            "24/24 [==============================] - 685s 29s/step - loss: 7.3625e-04 - accuracy: 1.0000 - val_loss: 0.1613 - val_accuracy: 0.9844\n",
            "Epoch 9/10\n",
            "24/24 [==============================] - 687s 29s/step - loss: 7.1922e-04 - accuracy: 1.0000 - val_loss: 0.1608 - val_accuracy: 0.9844\n",
            "Epoch 10/10\n",
            "24/24 [==============================] - 742s 31s/step - loss: 5.8474e-04 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9844\n",
            "Epoch 1/10\n",
            "24/24 [==============================] - 306s 12s/step - loss: 4.3146 - accuracy: 0.5408 - val_loss: 0.5013 - val_accuracy: 0.7552\n",
            "Epoch 2/10\n",
            "24/24 [==============================] - 307s 13s/step - loss: 0.5852 - accuracy: 0.7403 - val_loss: 0.5043 - val_accuracy: 0.7396\n",
            "Epoch 3/10\n",
            "24/24 [==============================] - 265s 11s/step - loss: 0.3710 - accuracy: 0.8514 - val_loss: 0.2887 - val_accuracy: 0.8906\n",
            "Epoch 4/10\n",
            "24/24 [==============================] - 272s 11s/step - loss: 0.3227 - accuracy: 0.8728 - val_loss: 0.2745 - val_accuracy: 0.8958\n",
            "Epoch 5/10\n",
            "24/24 [==============================] - 300s 13s/step - loss: 0.3542 - accuracy: 0.8380 - val_loss: 0.2944 - val_accuracy: 0.8854\n",
            "Epoch 6/10\n",
            "24/24 [==============================] - 299s 12s/step - loss: 0.2947 - accuracy: 0.8822 - val_loss: 0.3692 - val_accuracy: 0.8438\n",
            "Epoch 7/10\n",
            "24/24 [==============================] - 285s 12s/step - loss: 0.2595 - accuracy: 0.9036 - val_loss: 0.2806 - val_accuracy: 0.8906\n",
            "Epoch 8/10\n",
            "24/24 [==============================] - 291s 12s/step - loss: 0.2360 - accuracy: 0.9130 - val_loss: 0.2982 - val_accuracy: 0.8958\n",
            "Epoch 9/10\n",
            "24/24 [==============================] - 258s 11s/step - loss: 0.2192 - accuracy: 0.9170 - val_loss: 0.2874 - val_accuracy: 0.9062\n",
            "Epoch 10/10\n",
            "24/24 [==============================] - 265s 11s/step - loss: 0.2329 - accuracy: 0.9143 - val_loss: 0.2869 - val_accuracy: 0.9062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the test images are in a directory 'test_photos/'\n",
        "test_directory_path = '/content/drive/MyDrive/test_photos/'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_directory_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False)  # No need to shuffle test data\n",
        "\n",
        "# Check if the test generator has loaded any data\n",
        "if test_generator.n == 0:\n",
        "    raise ValueError(\"No images found in test directory\")\n",
        "\n",
        "# Calculate the correct number of steps for the predict method\n",
        "test_steps = ceil(test_generator.n / test_generator.batch_size)\n",
        "\n",
        "# Predict with VGG16 model\n",
        "vgg16_predictions = vgg16_model.predict(test_generator, steps=test_steps)\n",
        "\n",
        "# Predict with ResNet50 model\n",
        "resnet50_predictions = resnet50_model.predict(test_generator, steps=test_steps)\n",
        "\n",
        "# Flatten predictions and truncate to match the number of labels\n",
        "vgg16_predictions = vgg16_predictions.flatten()[:test_generator.n]\n",
        "resnet50_predictions = resnet50_predictions.flatten()[:test_generator.n]\n",
        "\n",
        "# Convert predictions to binary (0 or 1)\n",
        "vgg16_predictions = [1 if x > 0.5 else 0 for x in vgg16_predictions]\n",
        "resnet50_predictions = [1 if x > 0.5 else 0 for x in resnet50_predictions]\n",
        "\n",
        "# True labels\n",
        "true_labels = test_generator.classes\n",
        "\n",
        "# Ensure the predictions and labels have the same length\n",
        "assert len(vgg16_predictions) == len(true_labels), \"Mismatch in VGG16 samples\"\n",
        "assert len(resnet50_predictions) == len(true_labels), \"Mismatch in ResNet50 samples\"\n",
        "\n",
        "# Evaluate models\n",
        "print(\"VGG16 Model Performance:\")\n",
        "print(classification_report(true_labels, vgg16_predictions))\n",
        "\n",
        "print(\"ResNet50 Model Performance:\")\n",
        "print(classification_report(true_labels, resnet50_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjxOOE2Ktbf6",
        "outputId": "cf2980fb-7c12-47fd-c788-82dd6e0323fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 332 images belonging to 2 classes.\n",
            "11/11 [==============================] - 216s 19s/step\n",
            "11/11 [==============================] - 72s 6s/step\n",
            "VGG16 Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.64      0.78       143\n",
            "           1       0.79      1.00      0.88       189\n",
            "\n",
            "    accuracy                           0.85       332\n",
            "   macro avg       0.89      0.82      0.83       332\n",
            "weighted avg       0.88      0.85      0.84       332\n",
            "\n",
            "ResNet50 Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.36      0.52       143\n",
            "           1       0.67      0.97      0.79       189\n",
            "\n",
            "    accuracy                           0.71       332\n",
            "   macro avg       0.78      0.67      0.65       332\n",
            "weighted avg       0.77      0.71      0.67       332\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)"
      ],
      "metadata": {
        "id": "PJUqYgGhbIh1",
        "outputId": "39a05a27-804a-49cd-b7c2-e80c18390607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chicken': 0, 'non_chicken': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFzFUfXpbI9j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
